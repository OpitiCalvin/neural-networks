{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Deep Learning to Predict Taxi Fares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH = Path('/data/taxi_fare/')\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "data = pd.read_csv(PATH/'train.csv').sample(n = 2000000, random_state = 40)\n",
    "\n",
    "# Whole dataframe\n",
    "# data = pd.read_csv(PATH/'train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(PATH/'test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lat_diff'] = abs(data['dropoff_latitude'] - data['pickup_latitude'])\n",
    "data['lon_diff'] = abs(data['dropoff_longitude'] - data['pickup_longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['lat_diff'] = abs(test['dropoff_latitude'] - test['pickup_latitude'])\n",
    "test['lon_diff'] = abs(test['dropoff_longitude'] - test['pickup_longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']:\n",
    "    print(f'{x}; Max train value: {data[x].max()}, Max test value: {test[x].max()}')\n",
    "    print(f'{x}; Min train value: {data[x].min()}, Min test value: {test[x].min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = data.shape[0]\n",
    "for x in ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']:\n",
    "    data = data[(data[x] > test[x].min()) & (data[x] < test[x].max())]\n",
    "    \n",
    "print(f'{n1 - data.shape[0]} rows removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[(data['lat_diff'] > 1) | (data['lon_diff'] > 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test[(test['lat_diff'] > 1) | (test['lon_diff'] > 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~((data['lat_diff'] > 1) | (data['lon_diff'] > 1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers by Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(data['fare_amount']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(data[data[\"fare_amount\"] > 300])} rides with a fare greater than $300.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(data['fare_amount'], 99.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Fairs less than \\$0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data['fare_amount'] < 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['fare_amount'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the key column because it is a unique identifier and is not predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data) == data['key'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns = ['key'])\n",
    "test = test.drop(columns = ['key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Time and Date Information\n",
    "\n",
    "Using the fastai structured library to add time and date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.structured import *\n",
    "\n",
    "pd.options.display.max_columns = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(data, 'pickup_datetime', drop = False, time = True)\n",
    "add_datepart(test, 'pickup_datetime', drop = False, time = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Distance Information\n",
    "\n",
    "Using `haversine` distance between two points on a sphere. Answer from: https://stackoverflow.com/a/29546836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Radius of Earth in km\n",
    "R = 6367 \n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.    \n",
    "    \n",
    "    source: https://stackoverflow.com/a/29546836\n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = R * c\n",
    "    return km\n",
    "\n",
    "def minkowski(x1, x2, y1, y2, p):\n",
    "    # Minkowski distance between two (x, y, z) points indicated by p\n",
    "    return ((abs(x2 - x1))**p + (abs(y2 - y1))**p) ** (1 / p)\n",
    "\n",
    "def distances(lon1, lat1, lon2, lat2):\n",
    "    # Convert to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    # Convert to cartesian with approximation\n",
    "    x1 = R * np.cos(lat1) * np.cos(lon1)\n",
    "    y1 = R * np.cos(lat1) * np.sin(lon1)\n",
    "    z1 = R * np.sin(lat1)\n",
    "    \n",
    "    x2 = R * np.cos(lat2) * np.cos(lon2)\n",
    "    y2 = R * np.cos(lat2) * np.cos(lon2)\n",
    "    z2 = R * np.sin(lat2)\n",
    "    \n",
    "    manhattan = minkowski(x1, x2, y1, y2, z1, z2, p = 1)\n",
    "    euclidean = minkowski(x1, x2, y1, y2, z1, z2, p = 2)\n",
    "    \n",
    "    return manhattan, euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['haversine'] = haversine_np(data['pickup_longitude'], data['pickup_latitude'],\n",
    "                         data['dropoff_longitude'], data['dropoff_latitude'])\n",
    "data['haversine'].plot.hist();\n",
    "plt.title('Haversine Distance in KM');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['haversine'] = haversine_np(test['pickup_longitude'], test['pickup_latitude'],\n",
    "                         test['dropoff_longitude'], test['dropoff_latitude'])\n",
    "test['haversine'].plot.hist();\n",
    "plt.title('Haversine Distance in KM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(x):\n",
    "    x = np.sort(x)\n",
    "    n = len(x)\n",
    "    y = np.arange(1, n + 1, 1) / n\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = ecdf(data['haversine'].sample(10000))\n",
    "plt.plot(xs, ys, '.');\n",
    "plt.xlabel('Haversine Distance'); plt.ylabel('Percentile'); \n",
    "plt.title('ECDF of Haversine');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['haversine'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(data['haversine'], 99.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['haversine'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(test['haversine'], 99.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['haversine'] < 10, 'haversine'].plot.hist();\n",
    "plt.title('Haversine Distance in KM');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['manhattan-distance'] = (abs(data['lat_diff']) + abs(data['lon_diff']))\n",
    "data['euclidean-distance'] = np.sqrt(np.sum(np.square([data['lat_diff'], data['lon_diff']]), axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['manhattan-distance'].plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['euclidean-distance'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['haversine-bin'] = pd.cut(data['haversine'], bins = list(range(11)))\n",
    "data['haversine-bin'].value_counts().plot.bar(color = 'g');\n",
    "plt.title('Haversine Distance Bins'); plt.xlabel('bin'); plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['manhattan-distance'] = (abs(test['lat_diff']) + abs(test['lon_diff']))\n",
    "test['euclidean-distance'] = np.sqrt(np.sum(np.square([test['lat_diff'], test['lon_diff']]), axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['haversine-bin'] = round(data['haversine'])\n",
    "test['haversine-bin'] = round(test['haversine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dist in data['haversine-bin'].unique():\n",
    "    sns.kdeplot(np.log(data.loc[data['haversine-bin'] == dist, 'fare_amount'] + 1), label = f'{dist} km')\n",
    "plt.xlabel('Log of Fare'); plt.ylabel('Density'); plt.title('Fare by Haversine Distance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in data['pickup_datetimeDayofweek'].unique():\n",
    "    sns.kdeplot(np.log(data.loc[data['pickup_datetimeDayofweek'] == day, 'fare_amount'] + 1), label = f'{day}')\n",
    "plt.xlabel('Log of Fare'); plt.ylabel('Density'); plt.title('Fare by Day of Week');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "for hour in data['pickup_datetimeHour'].unique():\n",
    "    sns.kdeplot(np.log(data.loc[data['pickup_datetimeHour'] == hour, 'fare_amount'] + 1), label = f'{hour}')\n",
    "plt.xlabel('Log of Fare'); plt.ylabel('Density'); plt.title('Fare by Hour of Day');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(data.pop('fare_amount'))\n",
    "log_y = np.log(1 + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y);\n",
    "plt.title(\"Fare Distribution\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(log_y);\n",
    "plt.title(\"Log of Fare\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Accuracy Functions for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers, losses, metrics\n",
    "from keras import backend as K\n",
    "\n",
    "# Custom loss function\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis = -1))\n",
    "\n",
    "# In units of competition\n",
    "def convert_error(y_true, y_pred):\n",
    "    return root_mean_squared_error(K.exp(y_true) - 1, K.exp(y_pred) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Scale between 0 and 1 for network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Fit on training data and scale test data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data.drop(columns = 'pickup_datetime'))\n",
    "scaled_test = scaler.transform(test.drop(columns = 'pickup_datetime'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, input_dim = scaled_data.shape[1], activation = 'relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(128, activation = 'relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(1, activation = None))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model with custom accuracy function\n",
    "\n",
    "\n",
    "Using mean absolute error for loss because it was more stable in training.\n",
    "Checkpoints are early stopping and model saving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr = 0.01),\n",
    "              loss = losses.mean_absolute_error,\n",
    "              metrics = [convert_error])\n",
    "\n",
    "from keras import callbacks\n",
    "\n",
    "callback_list = [callbacks.EarlyStopping(monitor = 'val_loss', patience = 2),\n",
    "                 callbacks.ModelCheckpoint(filepath = 'model.ckpt', monitor = 'val_loss', save_best_only = True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and validation set based on binned fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, max(log_y), 6)\n",
    "\n",
    "binned_log_y = np.digitize(log_y, bins)\n",
    "\n",
    "for i in range(4):\n",
    "    print(f'Log y: {log_y[i]}, bin: {binned_log_y[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(binned_log_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to have at least two observations in every bin for stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_log_y[np.where(binned_log_y == 6)[0][0]] = 5\n",
    "plt.hist(binned_log_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(scaled_data, log_y, random_state = 40,\n",
    "                                                      stratify = binned_log_y, test_size = 250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of training: ', X_train.shape[0])\n",
    "print('Length of testing:  ', X_valid.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on the data\n",
    "model.fit(X_train, y_train, batch_size = 16, epochs = 25, verbose = 1, \n",
    "          callbacks = callback_list, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load back in best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('model.ckpt', compile=False)\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(),\n",
    "               loss = root_mean_squared_error,\n",
    "               metrics = [root_mean_squared_error, convert_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vloss, vlogrmse, vrmse = model.evaluate(X_valid, y_valid)\n",
    "print(f'Model validation RMSE: {round(vrmse, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model, metric_name):\n",
    "    \"Plot history of a keras model\"\n",
    "    \n",
    "    history = model.history.history\n",
    "    val_loss = history['val_loss']\n",
    "    train_loss = history['loss']\n",
    "    \n",
    "    train = history[metric_name]\n",
    "    val = history[f'val_{metric_name}']\n",
    "    \n",
    "    plt.style.use('fivethirtyeight')\n",
    "    plt.figure(figsize = (18, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(val_loss, color = 'b', label = 'val')\n",
    "    plt.plot(train_loss, color = 'r', label = 'train')\n",
    "    plt.xlabel('iteration'); plt.title('Loss');\n",
    "    plt.legend(prop = {'size': 18}, loc = 1)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val, color = 'b', label = 'val')\n",
    "    plt.plot(train, color = 'r', label = 'train')\n",
    "    plt.xlabel('iteration'); plt.title(f'{metric_name.capitalize()}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(model, metric_name = 'root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_predictions = model.predict(test_scaled)\n",
    "preds = (np.exp(log_predictions) - 1).reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'key': test_id,\n",
    "                           'fare_amount': list(preds)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['fare_amount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(submission['fare_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_lnk = PATH/'tmp/sub.csv'\n",
    "submission.to_csv(tmp_lnk, index = False)\n",
    "FileLink(tmp_lnk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical Variable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyc_encode(df, col, period):\n",
    "    \"\"\"Cyclical encoding of time series variables\"\"\"\n",
    "    df[f'{col}-sin'] = np.sin( (2 * np.pi * df[col]) / period)\n",
    "    df[f'{col}-cos'] = np.cos( (2 * np.pi * df[col]) / period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyc_encode(data, 'pickup_datetimeMonth', 12)\n",
    "cyc_encode(data, 'pickup_datetimeWeek', 52)\n",
    "cyc_encode(data, 'pickup_datetimeDay', 31)\n",
    "cyc_encode(data, 'pickup_datetimeDayofweek', 6)\n",
    "cyc_encode(data, 'pickup_datetimeDayofyear', 366)\n",
    "cyc_encode(data, 'pickup_datetimeHour', 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyc_encode(test, 'pickup_datetimeMonth', 12)\n",
    "cyc_encode(test, 'pickup_datetimeWeek', 52)\n",
    "cyc_encode(test, 'pickup_datetimeDay', 31)\n",
    "cyc_encode(test, 'pickup_datetimeDayofweek', 6)\n",
    "cyc_encode(test, 'pickup_datetimeDayofyear', 366)\n",
    "cyc_encode(test, 'pickup_datetimeHour', 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(data.drop(columns = 'pickup_datetime'))\n",
    "scaled_test = scaler.transform(test.drop(columns = 'pickup_datetime'))\n",
    "\n",
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, input_dim = input_dim, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(128, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(1, activation = None))\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(),\n",
    "              loss = losses.mean_absolute_error,\n",
    "              metrics = [convert_error])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(scaled_data, log_y, \n",
    "                                                      stratify = binned_log_y)\n",
    "\n",
    "callback_list = [callbacks.EarlyStopping(monitor = 'val_loss', patience = 2),\n",
    "                 callbacks.ModelCheckpoint(filepath = 'model_cyc.ckpt', monitor = 'val_loss', save_best_only = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(X_train.shape[1])\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 25, batch_size = 32, \n",
    "          verbose = 1, callbacks = callback_list, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vloss, vlogrmse, vrmse = model.evaluate(X_valid, y_valid)\n",
    "print(f'Model validation RMSE: {round(vrmse, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(model, 'root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_predictions = model.predict(scaled_test)\n",
    "preds = (np.exp(log_predictions) - 1).reshape((-1))\n",
    "\n",
    "submission = pd.DataFrame({'key': test_id,\n",
    "                           'fare_amount': list(preds)})\n",
    "\n",
    "tmp_lnk = PATH/'tmp/sub_cyc.csv'\n",
    "submission.to_csv(tmp_lnk, index = False)\n",
    "FileLink(tmp_lnk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(submission['fare_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test['haversine'] > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['haversine'] > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Embedddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of binary variables and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = list(data.select_dtypes(bool).columns)\n",
    "cat_vars = ['passenger_count'] + [x for x in data if (x.startswith('pickup_datetime') and (x not in binary_cols) and ('sin' not in x) and ('cos' not in x))]\n",
    "cat_vars.remove('pickup_datetime')\n",
    "cat_vars.remove('pickup_datetimeElapsed')\n",
    "cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_var in cat_vars:\n",
    "    print(f'Variable: {cat_var:{23}}\\tNumber of unique categories: {data[cat_var].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models\n",
    "\n",
    "Have to use the functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_out = []\n",
    "cat_in = []\n",
    "\n",
    "# Iterate through each variable\n",
    "for cat_var in cat_vars:\n",
    "    n_unique = data[cat_var].nunique()\n",
    "    \n",
    "    # Embedding shape from paper\n",
    "    embed = min((n_unique + 1) // 2, 50)\n",
    "    # One column input \n",
    "    model_in = layers.Input(shape = [1], name = f'{cat_var}-in')\n",
    "    cat_in.append(model_in)\n",
    "    \n",
    "    # Embedding layer\n",
    "    model_embed = layers.Embedding(n_unique + 1, embed, name = f'{cat_var}-embed')(model_in)\n",
    "    \n",
    "    # Reshape to one column\n",
    "    model_out = layers.Reshape(target_shape = [embed], name = f'{cat_var}-out')(model_embed)\n",
    "    cat_out.append(model_out)\n",
    "    \n",
    "    # model = models.Model(model_in, model_out)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rest = data.drop(columns = cat_vars + ['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rest_in = layers.Input(shape = [data_rest.shape[1]], name = f'rest-in')\n",
    "model_rest_out = layers.Dense(16, activation = 'relu', name = 'rest-out')(model_rest_in)\n",
    "model_rest = models.Model(model_rest_in, model_rest_out)\n",
    "\n",
    "model_ins = cat_in + [model_rest_in]\n",
    "model_outs = cat_out + [model_rest_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import concatenate\n",
    "concatenated = concatenate(model_outs, name = 'concatenate')\n",
    "concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final shape is 200, including 184 embedding dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build up rest of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = layers.Dense(16, input_shape = concatenated.get_shape(), activation = 'relu', name = 'dense-1-16')(concatenated)\n",
    "\n",
    "for i, n in enumerate([16, 32, 64, 128, 256]):\n",
    "    dense = layers.Dense(n, activation = 'relu', name = f'dense-{i + 2}-{n}')(dense)\n",
    "    dense = layers.Dropout(0.5)(dense)\n",
    "    \n",
    "out = layers.Dense(1, activation = None, name = 'prediction')(dense)\n",
    "overall_model = models.Model(model_ins, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of parameters in embedding for day of year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "( 366 * 50)  + 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of parameters in dense-6-64 layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(64 * 128) + 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = [callbacks.EarlyStopping(monitor = 'val_loss', patience = 2),\n",
    "                 callbacks.ModelCheckpoint(filepath = 'model_embed', monitor = 'val_loss', save_best_only = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_inputs = [np.array(data[cat_var]).reshape((-1)) for cat_var in cat_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = cat_inputs + [np.array(data_rest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dumps({'config': overall_model.get_config()}).decode('raw_unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_model.compile(optimizer=optimizers.Adam(), loss=losses.mean_absolute_error,\n",
    "                      metrics = [convert_error])\n",
    "\n",
    "overall_model.fit(all_inputs, log_y, epochs = 10, verbose = 1, batch_size = 1024,\n",
    "                  callbacks=[callbacks.EarlyStopping(monitor = 'val_loss', patience = 2)], validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var = cat_vars[1]\n",
    "df[cat_var].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_in = layers.Input(shape = [1])\n",
    "model1_out = layers.Embedding(8, 4)(model1_in)\n",
    "model1_out = layers.Reshape(target_shape = [4])(model1_out)\n",
    "model1 = models.Model(model1_in, model1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_in = layers.Input(shape = [df.shape[1]])\n",
    "model2_out = layers.Dense(16, activation = 'relu')(model2_in)\n",
    "model2 = models.Model(model2_in, model2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "concatenated = concatenate([model1_out, model2_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Dense(1, activation = None)(concatenated)\n",
    "merged_model = models.Model([model1_in, model2_in], out)\n",
    "merged_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.compile(optimizer = optimizers.Adam(),\n",
    "                     metrics = [metrics.mean_absolute_error, root_mean_squared_error],\n",
    "                     loss = losses.mean_absolute_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.fit([df['pickup_datetimeYear'], df], y = log_y, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "model_list = []\n",
    "\n",
    "for cat_var in cat_vars:\n",
    "    model = models.Sequential()\n",
    "    no_of_unique = df[cat_var].nunique()\n",
    "    embedding_size = min((no_of_unique + 1) // 2, 50)\n",
    "    embedding_size = int(embedding_size)\n",
    "    \n",
    "    # Add the embedding layer\n",
    "    model.add(layers.Embedding(no_of_unique + 1, embedding_size, input_length = 1))\n",
    "    \n",
    "    # Reshape to the embedding size\n",
    "    model.add(layers.Reshape(target_shape = ([embedding_size])))\n",
    "    model_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list[1].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rest = models.Sequential()\n",
    "model_rest.add(layers.Dense(16, input_dim = df.shape[1], activation = 'relu'))\n",
    "model_rest.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = concatenate([x for x in model_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list.append(model_rest)\n",
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Concatenate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = models.Sequential()\n",
    "full_model.add(layers.Concatenate()(model_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.add(Dense(1024))\n",
    "full_model.add(Activation('relu'))\n",
    "full_model.add(Dense(512))\n",
    "full_model.add(Activation('relu'))\n",
    "full_model.add(Dense(256))\n",
    "full_model.add(Activation('sigmoid'))\n",
    "\n",
    "full_model.add(Dense(2))\n",
    "full_model.add(Activation('sigmoid'))\n",
    "full_model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.fit(data, log_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "gmaps = googlemaps.Client(key='AIzaSyCTCV20ig7OJskXHp34oZCjk7V_t6yKNkQ')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "data['pickup'] = data['pickup_latitude'].astype(str) + \",\" + data['pickup_longitude'].astype(str)\n",
    "data['dropoff'] = data['dropoff_latitude'].astype(str) + \",\" + data['dropoff_longitude'].astype(str)\n",
    "\n",
    "def row_proc(pickup, dropoff):\n",
    "    geocode_result = gmaps.distance_matrix(pickup,dropoff)\n",
    "    #print (geocode_result)\n",
    "    try:\n",
    "        distance = float(geocode_result['rows'][0]['elements'][0]['distance']['text'].split()[0])\n",
    "        duration = geocode_result['rows'][0]['elements'][0]['duration']['text'].split()\n",
    "        if len(duration)==4:\n",
    "            mins = float(duration[0])*60 + float(duration[2])\n",
    "        else:\n",
    "            mins = float(duration[0])\n",
    "    except:\n",
    "        mins = np.nan\n",
    "        distance = np.nan\n",
    "    return pd.Series((distance, mins))\n",
    "\n",
    "data[['distance','duration']] = data.progress_apply(lambda row: row_proc(row.pickup, row.dropoff), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(x):\n",
    "    return np.sqrt(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
